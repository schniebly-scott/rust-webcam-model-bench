use std::sync::atomic::Ordering;
use std::time::Duration;
use std::{time::Instant, error::Error};
use std::sync::{Arc, Mutex};

use crate::SharedFrame;
use crate::config::ModelConfig;
use crate::cv::cv_worker::CVWorker;
use crate::utils::{ManagedService, ServiceCore};
use super::{Inference, cv_inference::Model};

#[derive(Debug)]
pub struct CVManager {
    config: ModelConfig,
    model: Arc<Mutex<Option<Model>>>,
    shared: SharedFrame,
    core: ServiceCore<Inference>,
}

impl CVManager {
    pub fn new(config: ModelConfig, shared: SharedFrame) -> Self {
        Self {
            config,
            model: Arc::new(Mutex::new(None)),
            shared,
            core: ServiceCore::new(1),
        }
    }

    pub fn load_model(&self) -> Result<Duration, Box<dyn Error>> {
        let now = Instant::now();
        let estimator = Model::new(&self.config.model_path)?;
        let elapsed = now.elapsed();

        let mut model_lock = self.model.lock().unwrap();
        *model_lock = Some(estimator);

        println!("Loading model took {:?}", elapsed);
        Ok(elapsed)
    }
}

impl ManagedService for CVManager {
    type Output = Inference;

    fn core(&self) -> &ServiceCore<Self::Output> {
        &self.core
    }

    fn start(&self) -> Result<(), Box<dyn std::error::Error>> {
        self.core.running.store(true, Ordering::SeqCst);

        CVWorker {
            config: self.config.clone(),
            model: self.model.clone(),
            shared: self.shared.clone(),
            core: self.core.clone(),
        }
        .spawn()
    }
}
